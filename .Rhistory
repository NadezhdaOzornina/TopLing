# Processing
library(tidyverse)
# Translation
library(polyglotr)
# Text analysis
library(quanteda)
library(stm)
# Plotting
library(VennDiagram)
library(grid)
# Calculations
library(proxy)
library(remotes)
library(gtools)
if (!requireNamespace("piercer", quietly = TRUE)) {
remotes::install_github("sjpierce/piercer")
}
library(piercer)
# Open and inspect the dataset
documents <- readRDS("documents.rds")
glimpse(documents)
cat("Mean document length is", mean(str_count(documents$original, "\\S+")))
# Define the translation function
translate <- function(docs_to_translate, lang_original, lang_translate, doc_ids) {
# Create a function for one document
translate_document <- function(doc, i) {
tryCatch({
res <- google_translate_long_text(doc, source = lang_original, target = lang_translate,
chunk_size = 1000,
preserve_newlines = FALSE)
message("Document ", i, " translated from ", lang_original, " into ", lang_translate)
res
}, error = function(e) {
message("Document ", i, " translation failed: ", e$message)
NA_character_
})
}
# Apply the function to all documents in the sample
translated <- Vectorize(translate_document, vectorize.args = c("doc", "i"))(
docs_to_translate, doc_ids
)
# Create a tibble and remove failed translations
tib <- tibble(
id = doc_ids,
original = docs_to_translate,
translated = translated
)
tib <- tib[!is.na(tib$translated), ]
tib
}
# Translate documents from German into English
#translation_forth <- translate(documents$text, "de", "en", documents$id)
# Translate documents from English back into German
#translation_back <- translate(translation_forth$translated, "en", "de", #translation_forth$id)
# Save translated data
#saveRDS(translation_forth, "translation_forth.rds")
#saveRDS(translation_back, "translation_back.rds")
# Read in translated data and stopwords
translation_forth <- readRDS("translation_forth.rds")
translation_back <- readRDS("translation_back.rds")
stopwords <- readRDS("stopwords.rds")
# Show data
cat("translation_forth:\n")
glimpse(translation_forth)
cat("translation_back:\n")
glimpse(translation_back)
cat("stopwords:\n")
glimpse(stopwords)
# Create preprocessing function
process_and_create_dfm <- function(translation_df, field, stopwords) {
# Create corpus
corpus <- translation_df %>%
corpus(text_field = field)
# Tokenize and preprocess
tokens_clean <- corpus %>%
tokens(what = "word", remove_punct = TRUE, remove_symbols = TRUE,
remove_numbers = TRUE, remove_url = TRUE,
remove_separators = TRUE, split_hyphens = TRUE,
split_tags = TRUE, include_docvars = FALSE) %>%
tokens_tolower() %>%
tokens_select(stopwords, selection = "remove") %>%
tokens_wordstem()
# 3. Create DFM and apply relative pruning
dfm_result <- dfm(tokens_clean)
dfm_result <- dfm_trim(dfm_result, min_docfreq = 0.005, max_docfreq = 0.99,
docfreq_type = 'prop', verbose = TRUE)
return(dfm_result)
}
# Prepare the data and define the numbers of topics to be examined
dfm_original_stm <- dfm_original %>%
convert(to = "stm")
# Apply preprocessing
dfm_original <- process_and_create_dfm(translation_forth, "original", stopwords) # original German texts
dfm_translated <- process_and_create_dfm(translation_back, "translated", stopwords) # translated back-and-forth
# Print preprocessing summary
cat("DFM from Original Texts (dfm_original):\n")
dfm_original
cat("DFM after Back-and-forth Translation (dfm_translated):\n")
dfm_translated
# Save the results
saveRDS(dfm_original, file = "dfm_original.rds")
saveRDS(dfm_translated, file = "dfm_translated.rds")
# Prepare the data and define the numbers of topics to be examined
dfm_original_stm <- dfm_original %>%
convert(to = "stm")
dfm_translated_stm <- dfm_translated %>%
convert(to = "stm")
Ks <- seq(5, 30, 5) # test for 5, 10, 15, 20, 25, 30 topics
# Apply function for metrics calculation from the STM package
kresult_original <- searchK(dfm_original_stm$documents, dfm_original_stm$vocab, K = Ks, init.type = "Spectral", verbose = FALSE)
library(devtools)
use_git()
use_github()
git remote -v
system("git add .")
system('git commit -m "Your commit message here"')
